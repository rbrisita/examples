{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/gemini/How_to_use_Gemini_Pro_API_with_WB_Weave.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{gemini-weave-intro} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_7u7Ft1-GXrK",
   "metadata": {},
   "source": [
    "# How to use Gemini Pro API with W&B Weave\n",
    "\n",
    "Read [our article](https://wandb.ai/prompt-eng/gemini-weave/reports/How-to-use-Gemini-Pro-API-with-W-B-Weave--Vmlldzo3NzEwNTA1) and follow along in this colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Nk3L-jqbGsrN",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdc0c0e-bc69-41ed-9b09-ec469a3a781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install google-generativeai weave -qqU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiI5pF6WGyd_",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import weave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Avj5TxGjFhE9",
   "metadata": {},
   "source": [
    "## Set up your Google API key and log into W&B Weave\n",
    "\n",
    "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0Lv-deDFgLl",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d8781-bea4-4f74-b1cd-b893f91bfee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init('prompt-eng/gemini-weave')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wFiQm3JAJpvw",
   "metadata": {},
   "source": [
    "## Generate a summary and track it in Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d096fc-309a-4c57-a3a8-926d88a520d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://raw.githubusercontent.com/wandb/llm-workshop-fc2024/main/part_2_structured_outputs/longpaper.txt\n",
    "with open('longpaper.txt', 'r') as file:\n",
    "    long_paper_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3343fcb9-b96d-478d-a4a8-fbb1e7b27a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = genai.get_model('models/gemini-1.5-pro-latest')\n",
    "print(model_info.input_token_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03216cd4-9fca-4dcb-aa73-b250cda7ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('models/gemini-1.5-pro-latest')\n",
    "model.count_tokens(long_paper_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32823a-f4f1-4233-8389-590e8757e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def generate_summary(text):\n",
    "    prompt = \"Generate a concise summary of below text:\\n\"\n",
    "    response = model.generate_content(prompt + long_paper_text)\n",
    "    return {\n",
    "        'summary': response.text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128199a-e13f-4d7e-b26c-55267ab8e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = generate_summary(long_paper_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ur0vXGNtJ5IF",
   "metadata": {},
   "source": [
    "## Gemini API JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6d4b7-f567-4070-b9d5-1867b72fc045",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-pro-latest\",\n",
    "                              generation_config={\"response_mime_type\": \"application/json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403ccf8-58fd-4e8a-b951-91e7aa39022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    title: str\n",
    "    summary: str = Field(description=\"plain short text summary without markdown\")\n",
    "\n",
    "schema = Summary.model_json_schema()\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49b8de-e40d-4dc5-8e79-97af5efb1797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b19a45-c4a9-47be-abda-97607f0e4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def create_prompt(text, schema):\n",
    "    prompt = f\"\"\"Generate a concise summary of below text using below JSON schema.\n",
    "Please output plain text without markdown and limit it to 200 words.\n",
    "Text:\n",
    "{text}\n",
    "JSON schema:\n",
    "{schema}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def generate_summary(text, schema):\n",
    "    prompt = create_prompt(text, schema)\n",
    "    response = model.generate_content(prompt)\n",
    "    try:\n",
    "        output = json.loads(response.text)\n",
    "    except:\n",
    "        output = response.text\n",
    "    return {\n",
    "        'summary': output\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b03f96-f04b-44b6-b03c-0065243408d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_summary = generate_summary(long_paper_text, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EFuMHhC5J8NO",
   "metadata": {},
   "source": [
    "## Evaluation with Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65abebe7-a0f4-4a7a-bdae-888d67e9403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import model_validator\n",
    "import os\n",
    "import time\n",
    "\n",
    "os.environ['WEAVE_PARALLELISM'] = '1' # remove parallelism due to our Gemini quota, remove it if not needed\n",
    "\n",
    "\n",
    "class SummaryModel(weave.Model):\n",
    "    model_name: str\n",
    "    prompt_template: str\n",
    "    json_schema: dict\n",
    "    model: genai.GenerativeModel\n",
    "\n",
    "    @model_validator(mode=\"before\")\n",
    "    def create_model(cls, v):\n",
    "        model_name = v[\"model_name\"]\n",
    "        model = genai.GenerativeModel(model_name,\n",
    "                generation_config={\"response_mime_type\": \"application/json\"})\n",
    "        v[\"model\"] = model\n",
    "        return v\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, text: str) -> dict:\n",
    "        time.sleep(15) # remove if your Gemini quota allows for it :)\n",
    "        prompt = self.prompt_template.format(text=text, schema=self.schema)\n",
    "        response = self.model.generate_content(prompt)\n",
    "        try:\n",
    "            output = json.loads(response.text)\n",
    "            return output[0]\n",
    "        except:\n",
    "            return {'summary': response.text}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8343cb-4a54-4fea-b2b0-b2c21b39e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Generate a concise summary of below text using below JSON schema.\n",
    "Please output plain text without markdown and limit it to 200 words.\n",
    "Text:\n",
    "{text}\n",
    "JSON schema:\n",
    "{schema}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69ed0c0-e032-4ced-8fd8-919ee332a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SummaryModel(model_name='gemini-1.5-pro-latest', prompt_template=prompt_template, \\\n",
    "                        json_schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d78ca87-bd61-45e6-b62c-0b82754d2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "await model.predict(long_paper_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5e402-0805-4888-8f08-0b28748fea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_uri = \"weave:///prompt-eng/gemini-weave/object/long_papers:9N9vkE4XY1SYoXLbvbCtP0YKqyqXErilG4XW8jYmQgE\"\n",
    "dataset = weave.ref(dataset_uri).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2f8804-dfd7-49b1-adbe-0a3dbb0c5ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring function checking format adherence\n",
    "@weave.op()\n",
    "def check_formatting(model_output: dict) -> dict:\n",
    "    # Check if length is smaller than threshold\n",
    "    result = False\n",
    "    if type(model_output) == list:\n",
    "        model_output = model_output[0]\n",
    "    if type(model_output) == dict:\n",
    "        if 'summary' in model_output.keys():\n",
    "            if type(model_output['summary']) == str:\n",
    "                result = True\n",
    "    return {'formatting': result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yfF8_ZsRMhYe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring function checking length of summary\n",
    "@weave.op()\n",
    "def check_conciseness(model_output: dict) -> dict:\n",
    "    # Check if length is smaller than threshold\n",
    "    result = False\n",
    "    if type(model_output) == list:\n",
    "        model_output = model_output[0]\n",
    "    if type(model_output) == dict:\n",
    "        if 'summary' in model_output.keys():\n",
    "            summary = model_output['summary']\n",
    "            if type(summary) == str:\n",
    "                result = len(summary.split()) < 300\n",
    "    return {'conciseness': result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c6bdb-83f9-4a63-99c4-0b2f0d3e9139",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(\n",
    "    dataset=dataset, scorers=[check_formatting, check_conciseness],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b2822-55cf-4983-a52f-10314ca79c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluation.evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba244582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39dc979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f8124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3012c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
