{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/diffusers/pixart-alpha-diffusers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation with Pixart-Î± using ðŸ¤— Diffusers\n",
    "\n",
    "This notebook demonstrates the following:\n",
    "- Performing text-conditional image-generations with the [Pixart-Î± model](https://huggingface.co/docs/diffusers/v0.23.1/en/api/pipelines/pixart) using [ðŸ¤— Diffusers](https://huggingface.co/docs/diffusers).\n",
    "- Manage image generation experiments using [Weights & Biases](http://wandb.ai/site).\n",
    "- Log the prompts, generated images and experiment configs to [Weigts & Biases](http://wandb.ai/site) for visalization.\n",
    "\n",
    "![](./assets/diffusers-autolog-1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers transformers accelerate sentencepiece ftfy wandb > install.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "from diffusers import PixArtAlphaPipeline\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.diffusers import autolog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained checkpoints from HuggingFace Hub to the PixArtAlphaPipeline\n",
    "pipe = PixArtAlphaPipeline.from_pretrained(\n",
    "    \"PixArt-alpha/PixArt-XL-2-1024-MS\", torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Enable offloading the weights to the CPU and only loading them on the GPU when\n",
    "# performing the forward pass can also save memory.\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_project = \"pixart-alpha\" # @param {type:\"string\"}\n",
    "\n",
    "prompt = \"a traveler navigating via a boat in countless mountains, Chinese ink painting\" # @param {type:\"string\"}\n",
    "negative_prompt = \"\" # @param {type:\"string\"}\n",
    "num_inference_steps = 25  # @param {type:\"slider\", min:10, max:50, step:1}\n",
    "guidance_scale = 4.5  # @param {type:\"slider\", min:0, max:10, step:0.1}\n",
    "num_images_per_prompt = 1 # @param {type:\"slider\", min:0, max:10, step:0.1}\n",
    "height = 1024 # @param {type:\"slider\", min:512, max:2560, step:32}\n",
    "width = 1024 # @param {type:\"slider\", min:512, max:2560, step:32}\n",
    "seed = None # @param {type:\"raw\"}\n",
    "\n",
    "\n",
    "def autogenerate_seed():\n",
    "    max_seed = int(1024 * 1024 * 1024)\n",
    "    seed = random.randint(1, max_seed)\n",
    "    seed = -seed if seed < 0 else seed\n",
    "    seed = seed % max_seed\n",
    "    return seed\n",
    "\n",
    "\n",
    "seed = autogenerate_seed() if seed is None else seed\n",
    "\n",
    "# Make the experiment reproducible by controlling randomness.\n",
    "# The seed would be automatically logged to WandB.\n",
    "generator = torch.Generator(device=\"cuda\").manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call WandB Autolog for Diffusers. This would automatically log\n",
    "# the prompts, generated images, pipeline architecture and all\n",
    "# associated experiment configs to Weights & Biases, thus making your\n",
    "# image generation experiments easy to reproduce, share and analyze.\n",
    "autolog(init=dict(project=wandb_project))\n",
    "\n",
    "# Generate the images by calling the PixArtAlphaPipeline\n",
    "image = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    num_images_per_prompt=num_images_per_prompt,\n",
    "    height=height,\n",
    "    width=width,\n",
    "    generator=generator,\n",
    ").images[0]\n",
    "\n",
    "# End the experiment\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
