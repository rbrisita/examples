{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv PDF Summarization Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import io\n",
    "import anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import base64\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "import weave\n",
    "from arxiv_models import convert_raw_arxiv_to_pydantic\n",
    "import filetype\n",
    "from PIL import Image\n",
    "import io\n",
    "from pdf2image import convert_from_bytes\n",
    "import PyPDF2\n",
    "from datetime import datetime, timezone\n",
    "from arxiv_models import ArxivPaper, Author, Link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(\"arxiv-papers-anthropic-testv4-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Anthropic anthropic_client\n",
    "anthropic_client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Fetch Arxiv Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def generate_arxiv_query_args(instruction, model=\"claude-3-sonnet-20240229\"):\n",
    "    tools = [{\n",
    "        \"name\": \"prepare_arxiv_search\",\n",
    "        \"description\": \"Prepare arguments for ArXiv paper search. This tool generates an optimal query string utilizing Boolean operators, field-specific syntax, and precise search terms. It also determines an efficient maximum number of results to fetch, balancing comprehensive coverage with processing efficiency. The output is tailored to the given research instruction, aiming to provide relevant and focused search results.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The ArXiv search query string. Supports Boolean operators (AND, OR, NOT), field-specific syntax (e.g., 'ti:' for title, 'au:' for author), quotation marks for exact phrases, and wildcards. Can include multiple search terms to refine results based on title, abstract, authors, comments, journal reference, subject category, or report number.\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"The maximum number of paper results to return from the ArXiv search. Aims to minimize the number of results while ensuring sufficient coverage of the topic. Defaults to 5 if not specified. Increasing this value broadens the search but may increase processing time and resource usage. Aim to be below 10 articles.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\", \"max_results\"]\n",
    "        }\n",
    "    }]\n",
    "\n",
    "    system_prompt = \"\"\"You are an expert at generating ArXiv queries. Use the prepare_arxiv_search tool to create an optimal query and determine the appropriate maximum number of results for the given research question. The query should utilize advanced search techniques including Boolean operators, field-specific syntax, and precise terms to ensure comprehensive yet focused results.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Use the prepare_arxiv_search tool to generate an optimal ArXiv query and determine the maximum number of results for the following research instruction: {instruction}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=4096,\n",
    "        messages=messages,\n",
    "        system=system_prompt,\n",
    "        tools=tools\n",
    "    )\n",
    "\n",
    "    # Extract the query and max_results from the response\n",
    "    for content in response.content:\n",
    "        if content.type == 'tool_use' and content.name == 'prepare_arxiv_search':\n",
    "            args = content.input\n",
    "            return args.get('query'), args.get('max_results')\n",
    "\n",
    "    # If no tool use was found, return a default query and the provided max_results\n",
    "    return f\"{instruction}\", 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction = \"Answer the following question: What are the latest advancements in audio music information retrieval?\"\n",
    "# arxiv_query, max_results = generate_arxiv_query_args(instruction)\n",
    "# print(f\"ArXiv query: {arxiv_query}\")\n",
    "# print(f\"Max results: {max_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def fetch_arxiv_papers(query, max_results=5):\n",
    "    # Initialize the arxiv Client\n",
    "    arxiv_client = arxiv.Client()\n",
    "    \n",
    "    # Create the search object\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # Fetch the results using client.results() and convert them to ArxivPaper objects\n",
    "    papers = []\n",
    "    for result in arxiv_client.results(search):\n",
    "        paper = convert_raw_arxiv_to_pydantic(result)\n",
    "        papers.append(paper)\n",
    "    \n",
    "    return papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv_papers = fetch_arxiv_papers(arxiv_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sample Arxiv paper object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper = ArxivPaper(\n",
    "    entry_id=\"http://arxiv.org/abs/2406.04744v1\",\n",
    "    updated=datetime(2024, 6, 7, 8, 43, 7, tzinfo=timezone.utc),\n",
    "    published=datetime(2024, 6, 7, 8, 43, 7, tzinfo=timezone.utc),\n",
    "    title=\"CRAG -- Comprehensive RAG Benchmark\",\n",
    "    authors=[\n",
    "        Author(full_name=\"Xiao Yang\"),\n",
    "        Author(full_name=\"Kai Sun\"),\n",
    "        Author(full_name=\"Hao Xin\"),\n",
    "        Author(full_name=\"Yushi Sun\"),\n",
    "        Author(full_name=\"Nikita Bhalla\"),\n",
    "        Author(full_name=\"Xiangsen Chen\"),\n",
    "        Author(full_name=\"Sajal Choudhary\"),\n",
    "        Author(full_name=\"Rongze Daniel Gui\"),\n",
    "        Author(full_name=\"Ziran Will Jiang\"),\n",
    "        Author(full_name=\"Ziyu Jiang\"),\n",
    "        Author(full_name=\"Lingkun Kong\"),\n",
    "        Author(full_name=\"Brian Moran\"),\n",
    "        Author(full_name=\"Jiaqi Wang\"),\n",
    "        Author(full_name=\"Yifan Ethan Xu\"),\n",
    "        Author(full_name=\"An Yan\"),\n",
    "        Author(full_name=\"Chenyu Yang\"),\n",
    "        Author(full_name=\"Eting Yuan\"),\n",
    "        Author(full_name=\"Hanwen Zha\"),\n",
    "        Author(full_name=\"Nan Tang\"),\n",
    "        Author(full_name=\"Lei Chen\"),\n",
    "        Author(full_name=\"Nicolas Scheffer\"),\n",
    "        Author(full_name=\"Yue Liu\"),\n",
    "        Author(full_name=\"Nirav Shah\"),\n",
    "        Author(full_name=\"Rakesh Wanga\"),\n",
    "        Author(full_name=\"Anuj Kumar\"),\n",
    "        Author(full_name=\"Wen-tau Yih\"),\n",
    "        Author(full_name=\"Xin Luna Dong\")\n",
    "    ],\n",
    "    summary=\"Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)'s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve <=34% accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracting thousands of participants and submissions within the first 50 days of the competition. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions.\",\n",
    "    comment=\"\",\n",
    "    journal_ref=None,\n",
    "    doi=\"10.48550/arXiv.2406.04744\",\n",
    "    primary_category=\"cs.CL\",\n",
    "    categories=[\"cs.CL\"],\n",
    "    links=[\n",
    "        Link(href=\"https://arxiv.org/abs/2406.04744\", title=\"Abstract\", rel=\"alternate\", content_type=None),\n",
    "        Link(href=\"https://arxiv.org/pdf/2406.04744\", title=\"pdf\", rel=\"related\", content_type=None)\n",
    "    ],\n",
    "    pdf_url=\"https://arxiv.org/pdf/2406.04744\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arxiv_paper.pdf_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(arxiv_result):\n",
    "    pdf_url = arxiv_result[\"pdf_url\"]\n",
    "    response = requests.get(pdf_url)\n",
    "    pdf_file = io.BytesIO(response.content)\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    return pdf_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Images to Text using Sonnet's vision capabilities\n",
    "\n",
    "Note: If we can't directly extract the image (in the case of SVGs or other vector graphics), we need to convert the page to an image first.\n",
    "Then we just ask the LLM to explain only the images on the page and to ignore the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vector_graphic_page_to_image(pdf_page, scale_factor=0.5):\n",
    "    def get_object(obj):\n",
    "        if isinstance(obj, PyPDF2.generic.IndirectObject):\n",
    "            return obj.get_object()\n",
    "        return obj\n",
    "\n",
    "    resources = get_object(pdf_page.get('/Resources', {}))\n",
    "    xobject = get_object(resources.get('/XObject', {}))\n",
    "    \n",
    "    # Check if there's a figure that's not an image\n",
    "    if xobject:\n",
    "        for obj in xobject.values():\n",
    "            obj = get_object(obj)\n",
    "            if isinstance(obj, dict) and obj.get('/Subtype') == '/Form':  # This indicates a vector graphic\n",
    "                # Convert the page to a PIL Image\n",
    "                pdf_bytes = io.BytesIO()\n",
    "                pdf_writer = PyPDF2.PdfWriter()\n",
    "                pdf_writer.add_page(pdf_page)\n",
    "                pdf_writer.write(pdf_bytes)\n",
    "                pdf_bytes.seek(0)\n",
    "                \n",
    "                # Convert PDF to image\n",
    "                images = convert_from_bytes(pdf_bytes.getvalue(), fmt='png')\n",
    "                \n",
    "                if images:\n",
    "                    image = images[0]\n",
    "                    # Resize the image\n",
    "                    new_size = (int(image.width * scale_factor), int(image.height * scale_factor))\n",
    "                    image = image.resize(new_size, Image.LANCZOS)\n",
    "                    img_byte_arr = io.BytesIO()\n",
    "                    image.save(img_byte_arr, format='PNG')\n",
    "                    img_byte_arr = img_byte_arr.getvalue()\n",
    "                    img_str = base64.b64encode(img_byte_arr).decode(\"utf-8\")\n",
    "                    data_url = f\"data:image/png;base64,{img_str}\"\n",
    "                    return data_url\n",
    "    \n",
    "    return None  # Return None if no conversion was needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage example:\n",
    "# pdf_reader = load_pdf(arxiv_paper)\n",
    "# page = pdf_reader.pages[3]\n",
    "# image = convert_vector_graphic_page_to_image(page)\n",
    "# if image:\n",
    "#     # Process the image as needed\n",
    "#     print(\"Image converted successfully\")\n",
    "# else:\n",
    "#     print(\"No vector graphics found or conversion failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def process_figure_image(data_url, model=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"Process image data and return a detailed technical description.\"\"\"\n",
    "    img_str = data_url.split(\",\")[1]\n",
    "\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=4096,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/png\",\n",
    "                            \"data\": img_str,\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"Analyze this image as if it's a figure from a scientific research paper. Provide a detailed technical description addressing the following:\n",
    "\n",
    "1. Type of figure (e.g., graph, diagram, flowchart, experimental setup)\n",
    "2. Key components or variables represented\n",
    "3. Relationships or trends depicted\n",
    "4. Quantitative information (if present)\n",
    "5. Methodology or process illustrated (if applicable)\n",
    "6. Potential implications or conclusions that can be drawn\n",
    "7. Any limitations or assumptions evident in the figure\n",
    "\n",
    "Focus on technical accuracy and relevance to scientific research. Avoid general descriptions and concentrate on the specific scientific content presented.\"\"\",\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def process_vector_image_pdf(data_url, model=\"claude-3-5-sonnet-20240620\"):\n",
    "    img_str = data_url.split(\",\")[1]\n",
    "\n",
    "    response = anthropic_client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=4096,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image\",\n",
    "                        \"source\": {\n",
    "                            \"type\": \"base64\",\n",
    "                            \"media_type\": \"image/png\",\n",
    "                            \"data\": img_str,\n",
    "                        },\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"This image is a full page from a scientific paper PDF, converted to PNG format. It may contain one or more vector graphic figures or charts. Your task is to:\n",
    "\n",
    "1. Identify and focus solely on the vector graphic figures or charts within the page.\n",
    "2. For each identified figure or chart, provide a detailed technical analysis addressing:\n",
    "\n",
    "   a. Type of figure (e.g., graph, diagram, flowchart)\n",
    "   b. Key components or variables represented\n",
    "   c. Relationships or trends depicted\n",
    "   d. Quantitative information (if present)\n",
    "   e. Methodology or process illustrated (if applicable)\n",
    "   f. Potential implications or conclusions that can be drawn\n",
    "\n",
    "3. Ignore any text or other elements on the page that are not part of the vector graphic figures.\n",
    "4. If multiple figures are present, analyze each separately and clearly indicate which figure you are describing.\n",
    "\n",
    "Focus on providing accurate, technical descriptions of the vector graphic content only.\"\"\",\n",
    "                    },\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def extract_images(paper, model=\"claude-3-5-sonnet-20240620\"):\n",
    "    \"\"\"Extract text and images from PDF content.\"\"\"\n",
    "\n",
    "    pdf_reader = load_pdf(paper)\n",
    "    all_images = []\n",
    "\n",
    "    for page in pdf_reader.pages:\n",
    "        images = []\n",
    "\n",
    "        for image in page.images:\n",
    "            img_data = image.data\n",
    "            kind = filetype.guess(img_data)\n",
    "            if kind is None:\n",
    "                print(f\"Cannot guess file type!\")\n",
    "                continue\n",
    "            \n",
    "            img_str = base64.b64encode(img_data).decode(\"utf-8\")\n",
    "            data_url = f\"data:{kind.mime};base64,{img_str}\"\n",
    "            try:\n",
    "                images.append(\n",
    "                    {\"image\": data_url, \"description\": process_figure_image(data_url, model=model)}\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image: {e}\")\n",
    "                images.append({\"image\": data_url, \"description\": \"\"})\n",
    "        \n",
    "        vector_graphics_image_data_url = convert_vector_graphic_page_to_image(page)\n",
    "        if vector_graphics_image_data_url:\n",
    "            images.append({\"image\": vector_graphics_image_data_url, \"description\": process_vector_image_pdf(vector_graphics_image_data_url, model=model)})\n",
    "        all_images.append(images)\n",
    "\n",
    "    return all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_images = extract_images(arxiv_paper)\n",
    "# extracted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def replace_images_with_descriptions(paper, images):\n",
    "    pdf_reader = load_pdf(paper)\n",
    "    text = \"\"\n",
    "    for page_num, page in enumerate(pdf_reader.pages):\n",
    "        text += page.extract_text() + \"\\n\\n\"\n",
    "        if images[page_num] and len(images[page_num]) > 0:\n",
    "            text += f\"\\n\\n[Image Descriptions for page {page_num+1}]\\n\"\n",
    "            for image_num, image in enumerate(images[page_num]):\n",
    "                text += f\"\\n[Image {image_num+1}]: {image['description']}\\n\"\n",
    "            text += \"[END OF IMAGE DESCRIPTIONS]\\n\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_text = replace_images_with_descriptions(arxiv_paper, extracted_images)\n",
    "# cleaned_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented Chain of Density Summarization\n",
    "1. Chunk and iteratively summarize the text\n",
    "2. Iteratively refine the final chunk-based summary\n",
    "3. Do one final pass of summarization to refine the density of the final summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Incorporate the question in the summary creation process instead of just using it to create the final summary\n",
    "@weave.op()\n",
    "def chain_of_density_summarization(instruction, text, model=\"claude-3-5-sonnet-20240620\", chunk_size=4000, chunk_iterations=2, density_iterations=2):\n",
    "    \"\"\"Apply Chain of Density summarization to the text with embedded image descriptions.\"\"\"\n",
    "    \n",
    "    @weave.op()\n",
    "    def chunk_text(text, chunk_size=4096):\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        lines = text.split('\\n')\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i]\n",
    "            if len(current_chunk) + len(line) > chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "            \n",
    "            current_chunk += line + \"\\n\"\n",
    "            \n",
    "            # Check if this line starts an image description section\n",
    "            if line.startswith(\"[Image Descriptions for page\"):\n",
    "                # If we have content before this, add it as a chunk\n",
    "                if current_chunk.strip():\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                    current_chunk = \"\"\n",
    "                \n",
    "                # Collect all image descriptions for this page\n",
    "                image_descriptions = line + \"\\n\"\n",
    "                i += 1\n",
    "                while i < len(lines) and not lines[i].startswith(\"[END OF IMAGE DESCRIPTIONS]\"):\n",
    "                    image_descriptions += lines[i] + \"\\n\"\n",
    "                    i += 1\n",
    "                if i < len(lines):\n",
    "                    image_descriptions += lines[i] + \"\\n\"\n",
    "                \n",
    "                # Add image descriptions as a separate chunk\n",
    "                chunks.append(image_descriptions.strip())\n",
    "                current_chunk = \"\"\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "    \n",
    "        # Combine chunks until they reach the defined chunk_size\n",
    "        combined_chunks = []\n",
    "        current_combined_chunk = \"\"\n",
    "        for chunk in chunks:\n",
    "            if len(current_combined_chunk) + len(chunk) <= chunk_size:\n",
    "                current_combined_chunk += chunk + \"\\n\\n\"\n",
    "            else:\n",
    "                if current_combined_chunk:\n",
    "                    combined_chunks.append(current_combined_chunk.strip())\n",
    "                current_combined_chunk = chunk + \"\\n\\n\"\n",
    "        \n",
    "        if current_combined_chunk:\n",
    "            combined_chunks.append(current_combined_chunk.strip())\n",
    "\n",
    "        return combined_chunks\n",
    "    \n",
    "    # Split the document into chunks\n",
    "    chunks = chunk_text(text, chunk_size)\n",
    "    print(f\"Number of chunks: {len(chunks)}\")\n",
    "    print(f\"Chunk sizes: {[len(chunk) for chunk in chunks]}\")\n",
    "            \n",
    "    @weave.op()\n",
    "    def summarize_chunk(chunk, instruction, current_summary=\"\", iteration=1):\n",
    "        prompt = f\"\"\"Current summary:\n",
    "        {current_summary}\n",
    "\n",
    "        New information:\n",
    "        {chunk}\n",
    "\n",
    "        Instruction to focus on: {instruction}\n",
    "\n",
    "        Iteration: {iteration}\n",
    "\n",
    "        Create an extremely dense, highly technical summary that specifically addresses the given instruction. Follow these steps:\n",
    "\n",
    "        1. Identify 3-5 key technical points from the new information that are directly relevant to the instruction, prioritizing:\n",
    "        - Novel methodologies or algorithms related to the instruction\n",
    "        - Specific quantitative results or metrics that address the instruction\n",
    "        - Detailed experimental setups or parameters pertinent to the instruction\n",
    "        - Precise definitions of domain-specific concepts mentioned in the instruction\n",
    "        - Critical limitations or assumptions in the research that affect the instruction\n",
    "\n",
    "        2. Integrate these points with the current summary, ensuring:\n",
    "        - Direct relevance to the instruction at hand\n",
    "        - No redundancy or oversimplification\n",
    "        - Preservation of technical nuances and complexities specific to the instruction\n",
    "        - Inclusion of relevant equations, formulas, or mathematical notations that help address the instruction\n",
    "        - Accurate representation of statistical significance and error margins for instruction-related data\n",
    "\n",
    "        3. Rephrase the combined information to maximize information density while maintaining focus on the instruction:\n",
    "        - Use domain-specific terminology and jargon without simplification, as relevant to the instruction\n",
    "        - Maintain the level of detail expected in a PhD-level discourse on the specific topic of the instruction\n",
    "        - Incorporate precise citations or references where applicable to support the response\n",
    "        - Preserve any conflicting viewpoints or ongoing debates in the field that relate to the instruction\n",
    "\n",
    "        4. With each iteration, aim to increase information density by 30-40% without sacrificing technical accuracy or critical details that address the instruction.\n",
    "\n",
    "        5. Ensure the summary includes instruction-specific:\n",
    "        - Methodological details (e.g., exact algorithms, parameter settings) that are crucial to addressing the instruction\n",
    "        - Precise quantitative results with appropriate units and error bounds that directly relate to the instruction\n",
    "        - Detailed descriptions of novel techniques or approaches that are key to addressing the instruction\n",
    "        - Critical analysis of strengths and limitations in the research as they pertain to the instruction\n",
    "\n",
    "        Produce a summary that is significantly more information-dense and technically precise than the previous one, while remaining laser-focused on addressing the given instruction. Use language appropriate for a highly specialized audience in the field.\"\"\"\n",
    "\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=4096,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "        return response.content[0].text\n",
    "    \n",
    "    @weave.op()\n",
    "    def summarize_current_summary(instruction, current_summary=\"\", iteration=1):\n",
    "        prompt = f\"\"\"Current summary:\n",
    "        {current_summary}\n",
    "\n",
    "        Instruction to focus on: {instruction}\n",
    "\n",
    "        Iteration: {iteration}\n",
    "\n",
    "        Generate an increasingly concise, entity-dense, and highly technical summary of the above text that specifically addresses the given instruction.\n",
    "\n",
    "        Follow these steps:\n",
    "        1. Identify 1-3 informative technical Entities from the original text which are missing from the current summary and are relevant to the instruction. These entities should be:\n",
    "        - Highly relevant to addressing the specific instruction\n",
    "        - Specific and technical (preferably 5 words or fewer)\n",
    "        - Novel (not in the current summary)\n",
    "        - Faithful (present in the original text)\n",
    "        - May include methodologies, algorithms, metrics, or key findings that directly relate to the instruction\n",
    "\n",
    "        2. Write a new, denser summary of identical length which covers every entity and technical detail from the current summary plus the newly identified Missing Entities, while maintaining focus on addressing the instruction.\n",
    "\n",
    "        Guidelines:\n",
    "        - Prioritize technical accuracy and specificity over general readability, always in the context of the given instruction.\n",
    "        - Make every word count: rewrite the current summary to improve information density and make space for additional technical entities that are relevant to the instruction.\n",
    "        - Use domain-specific terminology, precise quantitative information, and technical jargon where appropriate and relevant to addressing the instruction.\n",
    "        - Employ fusion, compression, and removal of less informative phrases to increase density, while ensuring all information pertains to the instruction.\n",
    "        - Ensure the summary remains highly dense and technical, yet self-contained and focused on the instruction.\n",
    "        - Never drop entities or technical details from the current summary that are relevant to the instruction. If space is limited, add fewer new entities.\n",
    "        - Maintain the exact same word count as the current summary.\n",
    "\n",
    "        Produce a summary that is more information-dense and technically precise than the previous one, suitable for an expert audience in the field, while remaining laser-focused on addressing the given instruction.\"\"\"\n",
    "\n",
    "        response = anthropic_client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=4096,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "\n",
    "\n",
    "    @weave.op()\n",
    "    def summarize_chunk_summaries(instruction, current_summary, chunk_summaries):\n",
    "        # Final densification step\n",
    "        return anthropic_client.messages.create(\n",
    "            model=\"claude-3-opus-20240229\", #Ensure it has a long context window\n",
    "            max_tokens=4096,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Given this current summary:\n",
    "\n",
    "        {current_summary}\n",
    "\n",
    "        And these chunk summaries:\n",
    "\n",
    "        {' '.join(chunk_summaries)}\n",
    "\n",
    "        And this instruction to focus on:\n",
    "\n",
    "        {instruction}\n",
    "\n",
    "        Create an extremely dense, final summary that refines the current summary by incorporating key information from the chunk summaries, while specifically addressing the given instruction. Follow these guidelines:\n",
    "\n",
    "        1. Integrate the most relevant and important information from the chunk summaries into the current summary.\n",
    "        2. Ensure all key technical content from both the current summary and chunk summaries that relates to the instruction is retained.\n",
    "        3. Aim to reduce overall length by 30-40% while increasing information density.\n",
    "        4. Prioritize highly specific methodologies, algorithms, metrics, and findings that directly address the instruction.\n",
    "        5. Preserve precise quantitative data, including statistical significance and error margins where applicable and relevant to the instruction.\n",
    "        6. Maintain the use of domain-specific terminology and technical jargon pertinent to the instruction.\n",
    "        7. Use compact phrasing and remove any remaining non-essential information that doesn't directly contribute to addressing the instruction.\n",
    "        8. If relevant to the instruction, include brief mentions of limitations, assumptions, or conflicting viewpoints from across all summaries.\n",
    "        9. Optimize for information density while maintaining coherence for an expert audience, always keeping the focus on the given instruction.\n",
    "\n",
    "        The final summary should be a highly concentrated, technical distillation of all provided summaries that specifically addresses the given instruction, suitable for specialists in the field.\"\"\",\n",
    "                    }\n",
    "                ],\n",
    "        ).content[0].text\n",
    "\n",
    "\n",
    "    @weave.op()\n",
    "    def summarize_chunk_iteration(chunks, instruction, current_summary, iteration):\n",
    "        chunk_summaries = []\n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            current_summary = summarize_chunk(chunk, instruction, current_summary, iteration)\n",
    "            chunk_summaries.append(current_summary)\n",
    "            print(f\"Iteration {iteration}, Chunk {i}:\\n{current_summary}\\n\")\n",
    "        current_summary = summarize_chunk_summaries(instruction, current_summary, chunk_summaries)\n",
    "        print(f\"Iteration {iteration}, Final Summary:\\n{current_summary}\\n\")\n",
    "        return current_summary, chunk_summaries\n",
    "\n",
    "    @weave.op()\n",
    "    def iterative_chunk_summarization(chunks, instruction, current_summary, chunk_iterations):\n",
    "        chunk_iteration_summaries = []\n",
    "        chunk_summaries = []\n",
    "        for iteration in range(1, chunk_iterations + 1):\n",
    "            current_summary, iteration_chunk_summaries = summarize_chunk_iteration(chunks, instruction, current_summary, iteration)\n",
    "            chunk_iteration_summaries.append(current_summary)\n",
    "            chunk_summaries.append(iteration_chunk_summaries)\n",
    "        return current_summary, chunk_iteration_summaries, chunk_summaries\n",
    "\n",
    "    current_summary, chunk_iteration_summaries, chunk_summaries = iterative_chunk_summarization(chunks, instruction, \"\", chunk_iterations)\n",
    "\n",
    "    @weave.op()\n",
    "    def iterative_density_summarization(instruction, current_summary, density_iterations):\n",
    "        iteration_summaries = []\n",
    "        for iteration in range(1, density_iterations + 1):\n",
    "            current_summary = summarize_current_summary(instruction, current_summary, iteration)\n",
    "            iteration_summaries.append(current_summary)\n",
    "            print(f\"Iteration {iteration}:\\n{current_summary}\\n\")\n",
    "        return current_summary, iteration_summaries\n",
    "\n",
    "    current_summary, iteration_summaries = iterative_density_summarization(instruction, current_summary, density_iterations)\n",
    "\n",
    "    @weave.op()\n",
    "    def final_summary(instruction, current_summary):\n",
    "        # Final densification step\n",
    "        return anthropic_client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=4096,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Given this summary:\n",
    "\n",
    "    {current_summary}\n",
    "\n",
    "    And this instruction to focus on:\n",
    "\n",
    "    {instruction}\n",
    "\n",
    "    Create an extremely dense, final summary that captures all key technical information in the most concise form possible, while specifically addressing the given instruction. Follow these guidelines:\n",
    "\n",
    "    1. Aim to reduce length by 30-40% while retaining all critical technical content relevant to the instruction.\n",
    "    2. Prioritize highly specific methodologies, algorithms, metrics, and findings that directly address the instruction.\n",
    "    3. Preserve precise quantitative data, including statistical significance and error margins where applicable and relevant to the instruction.\n",
    "    4. Maintain the use of domain-specific terminology and technical jargon pertinent to the instruction.\n",
    "    5. Ensure that all key entities and concepts from the original summary that relate to the instruction are represented.\n",
    "    6. Use compact phrasing and remove any remaining non-essential information that doesn't directly contribute to addressing the instruction.\n",
    "    7. If relevant to the instruction, include brief mentions of limitations, assumptions, or conflicting viewpoints.\n",
    "    8. Optimize for information density while maintaining coherence for an expert audience, always keeping the focus on the given instruction.\n",
    "\n",
    "    The final summary should be a highly concentrated, technical distillation of the research that specifically addresses the given instruction, suitable for specialists in the field.\"\"\",\n",
    "                }\n",
    "            ],\n",
    "        ).content[0].text\n",
    "\n",
    "    final_summary = final_summary(instruction, current_summary)\n",
    "    print(f\"Final Summary:\\n{final_summary}\\n\")\n",
    "\n",
    "    return {\n",
    "        \"final_summary\": final_summary,\n",
    "        \"accumulated_summary\": current_summary,\n",
    "        \"iteration_summaries\": iteration_summaries,\n",
    "        \"chunk_iteration_summaries\": chunk_iteration_summaries,\n",
    "        \"chunk_summaries\": chunk_summaries \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Explain the unique evaluation value props this RAG benchmark provides to AI Engineers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summaries = chain_of_density_summarization(question, cleaned_text)\n",
    "# print(summaries[\"final_summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Weave Model Object to better serialize the model for experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArxivChainOfDensityPipeline(weave.Model):\n",
    "\n",
    "    model: str = \"claude-3-5-sonnet-20240620\"\n",
    "    chunk_size: int = 20000\n",
    "    chunk_iterations: int = 1\n",
    "    density_iterations: int = 3\n",
    "\n",
    "    def __init__(self, model: str = \"claude-3-5-sonnet-20240620\", chunk_size: int = 4000, chunk_iterations: int = 1, density_iterations: int = 3):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_iterations = chunk_iterations\n",
    "        self.density_iterations = density_iterations\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, paper: ArxivPaper, instruction: str) -> dict:\n",
    "        extracted_images = extract_images(paper)\n",
    "        cleaned_text = replace_images_with_descriptions(paper, extracted_images)\n",
    "        return chain_of_density_summarization(instruction, cleaned_text, model=self.model, chunk_size=self.chunk_size, chunk_iterations=self.chunk_iterations, density_iterations=self.density_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_chain_of_density_pipeline = ArxivChainOfDensityPipeline()\n",
    "# arxiv_chain_of_density_pipeline.predict(arxiv_paper, \"Determine how I would best incorporate these benchmarks for my customer support RAG system. What evaluations would work best specifically for me?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and evaluate the experiments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper1 = ArxivPaper(\n",
    "    entry_id=\"http://arxiv.org/abs/2405.05904\",\n",
    "    updated=datetime(2024, 5, 13, 7, 29, 58, tzinfo=timezone.utc),\n",
    "    published=datetime(2024, 5, 9, 17, 0, 22, tzinfo=timezone.utc),\n",
    "    title=\"Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?\",\n",
    "    authors=[\n",
    "        Author(full_name=\"Zorik Gekhman\"),\n",
    "        Author(full_name=\"Gal Yona\"),\n",
    "        Author(full_name=\"Roee Aharoni\"),\n",
    "        Author(full_name=\"Matan Eyal\"),\n",
    "        Author(full_name=\"Amir Feder\"),\n",
    "        Author(full_name=\"Roi Reichart\"),\n",
    "        Author(full_name=\"Jonathan Herzig\")\n",
    "    ],\n",
    "    summary=(\"When large language models are aligned via supervised fine-tuning, they may encounter new factual information \"\n",
    "             \"that was not acquired through pre-training. It is often conjectured that this can teach the model the behavior \"\n",
    "             \"of hallucinating factually incorrect responses, as the model is trained to generate facts that are not grounded \"\n",
    "             \"in its pre-existing knowledge. In this work, we study the impact of such exposure to new knowledge on the capability \"\n",
    "             \"of the fine-tuned model to utilize its pre-existing knowledge. To this end, we design a controlled setup, focused on \"\n",
    "             \"closed-book QA, where we vary the proportion of the fine-tuning examples that introduce new knowledge. We demonstrate \"\n",
    "             \"that large language models struggle to acquire new factual knowledge through fine-tuning, as fine-tuning examples that \"\n",
    "             \"introduce new knowledge are learned significantly slower than those consistent with the model's knowledge. However, we \"\n",
    "             \"also find that as the examples with new knowledge are eventually learned, they linearly increase the model's tendency \"\n",
    "             \"to hallucinate. Taken together, our results highlight the risk in introducing new factual knowledge through fine-tuning, \"\n",
    "             \"and support the view that large language models mostly acquire factual knowledge through pre-training, whereas fine-tuning \"\n",
    "             \"teaches them to use it more efficiently.\"),\n",
    "    comment=None,\n",
    "    journal_ref=None,\n",
    "    doi=\"10.48550/arXiv.2405.05904\",\n",
    "    primary_category=\"cs.CL\",\n",
    "    categories=[\"cs.CL\"],\n",
    "    links=[\n",
    "        Link(href=\"https://arxiv.org/abs/2405.05904\", title=\"Abstract\", rel=\"alternate\"),\n",
    "        Link(href=\"https://arxiv.org/pdf/2405.05904\", title=\"pdf\", rel=\"related\")\n",
    "    ],\n",
    "    pdf_url=\"https://arxiv.org/pdf/2405.05904\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper2 = ArxivPaper(\n",
    "    entry_id=\"http://arxiv.org/abs/2404.11018\",\n",
    "    updated=datetime(2024, 5, 22, 17, 6, 10, tzinfo=timezone.utc),\n",
    "    published=datetime(2024, 4, 17, 2, 49, 26, tzinfo=timezone.utc),\n",
    "    title=\"Many-Shot In-Context Learning\",\n",
    "    authors=[\n",
    "        Author(full_name=\"Rishabh Agarwal\"),\n",
    "        Author(full_name=\"Avi Singh\"),\n",
    "        Author(full_name=\"Lei M. Zhang\"),\n",
    "        Author(full_name=\"Bernd Bohnet\"),\n",
    "        Author(full_name=\"Luis Rosias\"),\n",
    "        Author(full_name=\"Stephanie Chan\"),\n",
    "        Author(full_name=\"Biao Zhang\"),\n",
    "        Author(full_name=\"Ankesh Anand\"),\n",
    "        Author(full_name=\"Zaheer Abbas\"),\n",
    "        Author(full_name=\"Azade Nova\"),\n",
    "        Author(full_name=\"John D. Co-Reyes\"),\n",
    "        Author(full_name=\"Eric Chu\"),\n",
    "        Author(full_name=\"Feryal Behbahani\"),\n",
    "        Author(full_name=\"Aleksandra Faust\"),\n",
    "        Author(full_name=\"Hugo Larochelle\")\n",
    "    ],\n",
    "    summary=(\"Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, \"\n",
    "             \"without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. \"\n",
    "             \"Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, \"\n",
    "             \"many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced \"\n",
    "             \"and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the \"\n",
    "             \"prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the \"\n",
    "             \"many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding \"\n",
    "             \"pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. Our analysis also reveals the limitations \"\n",
    "             \"of next-token prediction loss as an indicator of downstream ICL performance.\"),\n",
    "    comment=None,\n",
    "    journal_ref=None,\n",
    "    doi=\"10.48550/arXiv.2404.11018\",\n",
    "    primary_category=\"cs.LG\",\n",
    "    categories=[\"cs.LG\", \"cs.AI\", \"cs.CL\"],\n",
    "    links=[\n",
    "        Link(href=\"https://arxiv.org/abs/2404.11018\", title=\"Abstract\", rel=\"alternate\"),\n",
    "        Link(href=\"https://arxiv.org/pdf/2404.11018\", title=\"pdf\", rel=\"related\")\n",
    "    ],\n",
    "    pdf_url=\"https://arxiv.org/pdf/2404.11018\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper3 = ArxivPaper(\n",
    "    entry_id=\"http://arxiv.org/abs/2406.18403\",\n",
    "    updated=datetime(2024, 6, 26, 14, 56, 13, tzinfo=timezone.utc),\n",
    "    published=datetime(2024, 6, 26, 14, 56, 13, tzinfo=timezone.utc),\n",
    "    title=\"LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks\",\n",
    "    authors=[\n",
    "        Author(full_name=\"Anna Bavaresco\"),\n",
    "        Author(full_name=\"Raffaella Bernardi\"),\n",
    "        Author(full_name=\"Leonardo Bertolazzi\"),\n",
    "        Author(full_name=\"Desmond Elliott\"),\n",
    "        Author(full_name=\"Raquel Fernández\"),\n",
    "        Author(full_name=\"Albert Gatt\"),\n",
    "        Author(full_name=\"Esam Ghaleb\"),\n",
    "        Author(full_name=\"Mario Giulianelli\"),\n",
    "        Author(full_name=\"Michael Hanna\"),\n",
    "        Author(full_name=\"Alexander Koller\"),\n",
    "        Author(full_name=\"André F. T. Martins\"),\n",
    "        Author(full_name=\"Philipp Mondorf\"),\n",
    "        Author(full_name=\"Vera Neplenbroek\"),\n",
    "        Author(full_name=\"Sandro Pezzelle\"),\n",
    "        Author(full_name=\"Barbara Plank\"),\n",
    "        Author(full_name=\"David Schlangen\"),\n",
    "        Author(full_name=\"Alessandro Suglia\"),\n",
    "        Author(full_name=\"Aditya K Surikuchi\"),\n",
    "        Author(full_name=\"Ece Takmaz\"),\n",
    "        Author(full_name=\"Alberto Testoni\")\n",
    "    ],\n",
    "    summary=(\"There is an increasing trend towards evaluating NLP models with LLM-generated judgments instead of human judgments. \"\n",
    "             \"In the absence of a comparison against human data, this raises concerns about the validity of these evaluations; in case they are conducted with proprietary models, \"\n",
    "             \"this also raises concerns over reproducibility. We provide JUDGE-BENCH, a collection of 20 NLP datasets with human annotations, and comprehensively evaluate 11 current LLMs, \"\n",
    "             \"covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show that each LLM exhibits a large variance across datasets in its correlation to human judgments. \"\n",
    "             \"We conclude that LLMs are not yet ready to systematically replace human judges in NLP.\"),\n",
    "    comment=None,\n",
    "    journal_ref=None,\n",
    "    doi=\"10.48550/arXiv.2406.18403\",\n",
    "    primary_category=\"cs.CL\",\n",
    "    categories=[\"cs.CL\"],\n",
    "    links=[\n",
    "        Link(href=\"https://arxiv.org/abs/2406.18403\", title=\"Abstract\", rel=\"alternate\"),\n",
    "        Link(href=\"https://arxiv.org/pdf/2406.18403\", title=\"pdf\", rel=\"related\")\n",
    "    ],\n",
    "    pdf_url=\"https://arxiv.org/pdf/2406.18403\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_paper1.pdf_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_papers = [\n",
    "    arxiv_paper1,\n",
    "    arxiv_paper2,\n",
    "    arxiv_paper3\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_instructions = [\n",
    "    \"Summarize the key methodologies and novel contributions of this research, focusing on their potential impact in the field.\",\n",
    "    # \"Analyze the experimental setup, results, and limitations of this study, highlighting any statistical significance and error margins.\",\n",
    "    # \"Compare this paper's approach to existing methods in the field, explaining how it addresses current challenges or limitations.\"\n",
    "]\n",
    "models = [\n",
    "    # \"claude-3-opus-20240229\",\n",
    "    \"claude-3-haiku-20240307\",\n",
    "    \"claude-3-5-sonnet-20240620\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = list(product(eval_papers, eval_instructions))\n",
    "print(len(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = weave.Dataset(name=\"we-paper-reading-eval-data\", rows=[{\"paper\": arxiv_paper, \"instruction\": instruction, \"summary\": arxiv_paper.summary} for arxiv_paper, instruction in eval_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.publish(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def quality_scorer(instruction, model_output, model=\"gpt-4o\"):\n",
    "    openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    def score_summary(summary, summary_type):\n",
    "        prompt = f\"\"\"Evaluate the quality of the following {summary_type} based on how well it addresses the given instruction. Use the scoring rules below to calculate three numerical scores between 0 and 10.\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "{summary_type}:\n",
    "{summary}\n",
    "\n",
    "Scoring Rules:\n",
    "1. Relevance (0-10):\n",
    "   - 10: Perfectly addresses all aspects of the instruction, focusing on key methodologies and novel contributions\n",
    "     Example: \"The paper introduces a novel Chain of Density (CoD) approach for text summarization, iteratively refining summaries to increase information density while maintaining coherence.\"\n",
    "   - 8-9: Addresses most aspects of the instruction with minor omissions\n",
    "     Example: \"The research presents a new summarization method called Chain of Density, which iteratively improves summary quality through multiple refinement steps.\"\n",
    "   - 6-7: Addresses the main points of the instruction but misses some details about methodologies or contributions\n",
    "     Example: \"The study proposes a new approach to text summarization that aims to increase information density in generated summaries.\"\n",
    "   - 4-5: Partially addresses the instruction, missing significant aspects of methodologies or contributions\n",
    "     Example: \"The paper discusses improvements in text summarization techniques, focusing on increasing information content.\"\n",
    "   - 2-3: Barely addresses the instruction, focusing on tangential information\n",
    "     Example: \"The research explores various natural language processing tasks, including text summarization.\"\n",
    "   - 0-1: Completely irrelevant to the instruction\n",
    "     Example: \"The paper discusses advancements in computer vision algorithms for image recognition.\"\n",
    "\n",
    "2. Technical Quality (0-10):\n",
    "   - 10: Exceptionally accurate, detailed, and technically sound, with precise descriptions of methodologies and contributions\n",
    "     Example: \"The Chain of Density approach employs a multi-step refinement process, using GPT-4 to iteratively increase information density. Each iteration aims to reduce length by 20-30% while preserving key information, resulting in progressively denser summaries.\"\n",
    "   - 8-9: Highly accurate with comprehensive technical details about research methods and findings\n",
    "     Example: \"The CoD method uses large language models to refine summaries over multiple iterations, with each step aiming to increase information density by 20-30% while maintaining coherence.\"\n",
    "   - 6-7: Generally accurate with good technical depth, but may lack some specifics\n",
    "     Example: \"The proposed summarization technique uses iterative refinement to increase information density in generated summaries, leveraging large language models in the process.\"\n",
    "   - 4-5: Mostly accurate but lacks important technical details about methodologies or contributions\n",
    "     Example: \"The study presents a new summarization method that aims to improve summary quality through multiple refinement steps.\"\n",
    "   - 2-3: Contains technical inaccuracies or lacks significant depth in describing research approaches\n",
    "     Example: \"The paper discusses a summarization technique that uses AI to make summaries shorter and more informative.\"\n",
    "   - 0-1: Technically unsound or extremely superficial in describing methodologies and contributions\n",
    "     Example: \"The research uses AI to make better summaries of text.\"\n",
    "\n",
    "3. Conciseness (0-10):\n",
    "   - 10: Maximally information-dense without any unnecessary content, perfectly balancing detail and brevity\n",
    "     Example: \"CoD: Novel iterative summarization method. GPT-4 refines summaries, increasing density 20-30% per iteration. Outperforms baselines in human evaluation.\"\n",
    "   - 8-9: Highly concise with minimal extraneous information, efficiently describing methodologies and contributions\n",
    "     Example: \"Chain of Density: Iterative summarization technique using GPT-4. Progressively increases information density while maintaining coherence. Superior to baselines in human studies.\"\n",
    "   - 6-7: Generally concise but could be slightly more compact in describing research approaches\n",
    "     Example: \"The paper introduces Chain of Density, a new summarization method that uses multiple refinement steps to increase information density in summaries. It shows improvements over existing techniques.\"\n",
    "   - 4-5: Contains some unnecessary information or repetition, diluting the focus on key methodologies and contributions\n",
    "     Example: \"The research presents a new approach to summarization called Chain of Density. This method aims to make summaries more informative by refining them multiple times. The authors conducted experiments to show that their method works better than other existing methods.\"\n",
    "   - 2-3: Verbose with significant redundancy, obscuring the main research points\n",
    "     Example: \"In this paper, the authors talk about a new way to make summaries of text. They call their method Chain of Density. The idea is to take a summary and make it better by refining it multiple times. They use AI to do this. They say their method is better than other methods that exist currently.\"\n",
    "   - 0-1: Extremely verbose or filled with irrelevant information unrelated to methodologies and contributions\n",
    "     Example: \"The researchers in this study were interested in natural language processing, which is a field of artificial intelligence that deals with how computers understand and generate human language. They looked at many different aspects of this field and eventually decided to focus on summarization, which is the task of making shorter versions of longer texts.\"\n",
    "\n",
    "Examples:\n",
    "\n",
    "1. High-quality summary (Instruction: \"Summarize the key methodologies and novel contributions of this research, focusing on their potential impact in the field.\"):\n",
    "{{\n",
    "    \"relevance\": {{\n",
    "        \"score\": 9.5\n",
    "    }},\n",
    "    \"technical_quality\": {{\n",
    "        \"score\": 9.0\n",
    "    }},\n",
    "    \"conciseness\": {{\n",
    "        \"score\": 8.5\n",
    "    }}\n",
    "}}\n",
    "\n",
    "2. Average-quality summary (Instruction: \"Analyze the experimental setup, results, and limitations of this study.\"):\n",
    "{{\n",
    "    \"relevance\": {{\n",
    "        \"score\": 6.0\n",
    "    }},\n",
    "    \"technical_quality\": {{\n",
    "        \"score\": 5.5\n",
    "    }},\n",
    "    \"conciseness\": {{\n",
    "        \"score\": 7.0\n",
    "    }}\n",
    "}}\n",
    "\n",
    "3. Low-quality summary (Instruction: \"Explain how this paper's approach compares to existing methods in the field.\"):\n",
    "{{\n",
    "    \"relevance\": {{\n",
    "        \"score\": 3.0\n",
    "    }},\n",
    "    \"technical_quality\": {{\n",
    "        \"score\": 2.5\n",
    "    }},\n",
    "    \"conciseness\": {{\n",
    "        \"score\": 4.0\n",
    "    }}\n",
    "}}\n",
    "\n",
    "Provide your evaluation in the following JSON format:\n",
    "{{\n",
    "    \"relevance\": {{\n",
    "        \"score\": <float>\n",
    "    }},\n",
    "    \"technical_quality\": {{\n",
    "        \"score\": <float>\n",
    "    }},\n",
    "    \"conciseness\": {{\n",
    "        \"score\": <float>\n",
    "    }}\n",
    "}}\n",
    "\n",
    "Ensure your response is ONLY valid JSON. Do not include any other text outside the JSON object.\n",
    "Ensure you have the keys: relevance, technical_quality, conciseness, each containing only a score.\n",
    "Ensure each score is a float between 0 and 10, using the scoring rules provided above.\n",
    "\"\"\"\n",
    "\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "\n",
    "    # ... rest of the function remains the same ...\n",
    "\n",
    "\n",
    "    scores = {\n",
    "        \"chunk_summaries\": [],\n",
    "        \"chunk_iteration_summaries\": [],\n",
    "        \"iteration_summaries\": [],\n",
    "        \"accumulated_summary\": {},\n",
    "        \"final_summary\": {}\n",
    "    }\n",
    "\n",
    "    # Score chunk summaries\n",
    "    for i, chunk_summary_list in enumerate(model_output[\"chunk_summaries\"]):\n",
    "        chunk_scores = []\n",
    "        for j, chunk_summary in enumerate(chunk_summary_list):\n",
    "            score = score_summary(chunk_summary, f\"Chunk Summary {i+1}.{j+1}\")\n",
    "            chunk_scores.append(score)\n",
    "        scores[\"chunk_summaries\"].append(chunk_scores)\n",
    "\n",
    "    # Score chunk iteration summaries\n",
    "    for i, summary in enumerate(model_output[\"chunk_iteration_summaries\"]):\n",
    "        score = score_summary(summary, f\"Chunk Iteration Summary {i+1}\")\n",
    "        scores[\"chunk_iteration_summaries\"].append(score)\n",
    "\n",
    "    # Score iteration summaries\n",
    "    for i, summary in enumerate(model_output[\"iteration_summaries\"]):\n",
    "        score = score_summary(summary, f\"Iteration Summary {i+1}\")\n",
    "        scores[\"iteration_summaries\"].append(score)\n",
    "\n",
    "    # Score accumulated summary\n",
    "    scores[\"accumulated_summary\"] = score_summary(model_output[\"accumulated_summary\"], \"Accumulated Summary\")\n",
    "\n",
    "    # Score final summary\n",
    "    scores[\"final_summary\"] = score_summary(model_output[\"final_summary\"], \"Final Summary\")\n",
    "\n",
    "    # Unpack list keys into JSON with key_index as new keys\n",
    "    for key in [\"chunk_summaries\", \"chunk_iteration_summaries\", \"iteration_summaries\"]:\n",
    "        unpacked = {}\n",
    "        for i, item in enumerate(scores[key]):\n",
    "            if isinstance(item, list):\n",
    "                for j, subitem in enumerate(item):\n",
    "                    unpacked[f\"{key}_{i+1}_{j+1}\"] = subitem\n",
    "            else:\n",
    "                unpacked[f\"{key}_{i+1}\"] = item\n",
    "        scores[key] = unpacked\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(dataset=dataset, scorers=[quality_scorer])\n",
    "for model in models:\n",
    "    arxiv_chain_of_density_pipeline = ArxivChainOfDensityPipeline(model=model)\n",
    "    await evaluation.evaluate(arxiv_chain_of_density_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
