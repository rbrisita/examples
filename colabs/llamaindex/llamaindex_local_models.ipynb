{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/llamaindex/llamaindex_local_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TL;DR:** Build a RAG application using llamaindex and local models (embedding + LLM), with [weave](https://wandb.github.io/weave/) for LLM observability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Packages and Basic Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://controlroom.jurassicoutpost.com/app/uploads/2016/05/JurassicPark-Final.pdf\n",
    "!pip install -qU llama-index-callbacks-wandb\n",
    "!pip install -qU llama-index-llms-huggingface\n",
    "!pip install -qU llama-index-readers-file pymupdf\n",
    "!pip install -qU llama-index-embeddings-huggingface\n",
    "!pip install -qU weave ml-collections accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import weave\n",
    "from llama_index.callbacks.wandb import WandbCallbackHandler\n",
    "\n",
    "wandb.login()\n",
    "weave.init(\"llamaindex-weave-jurassic-qna\")\n",
    "wandb_callback = WandbCallbackHandler(\n",
    "    run_args={\"project\": \"llamaindex-weave-jurassic-qna\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title ‚öôÔ∏è Configuration\n",
    "import ml_collections\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "def get_config() -> ml_collections.ConfigDict:\n",
    "    config = ml_collections.ConfigDict()\n",
    "    config.model: str = \"Writer/camel-5b-hf\"  # @param {type: \"string\"}\n",
    "    config.embedding_model: str = \"BAAI/bge-small-en-v1.5\"  # @param {type: \"string\"}\n",
    "    config.fetch_index_from_wandb: bool = True  # @param {type: \"boolean\"}\n",
    "    config.wandb_entity: str = \"sauravmaheshkar\"  # @param {type: \"string\"}\n",
    "\n",
    "    return config\n",
    "\n",
    "\n",
    "config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíø The Dataset\n",
    "---\n",
    "\n",
    "In this example, we'll use the original Jurassic Park screenplay to act as our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "documents = PyMuPDFReader().load(\n",
    "    file_path=\"/content/JurassicPark-Final.pdf\", metadata=True\n",
    ")\n",
    "\n",
    "doc_text = \"\\n\\n\".join([d.get_content() for d in documents])\n",
    "docs = [Document(text=doc_text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úçÔ∏è Model Architecture & Training\n",
    "---\n",
    "\n",
    "Since we're using all local models in this example, we'll have to our own Embedding model and llm. In this particular example we'll use \"`BAAI/bge-small-en-v1.5`\" as our local embedding model and \"`Writer/camel-5b-hf`\" as the local LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=config.embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
    ")\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"do_sample\": False},\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=config.model,\n",
    "    model_name=config.model,\n",
    "    device_map=\"auto\",\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇ Creating a Index\n",
    "---\n",
    "\n",
    "Based on the value you set for `config.fetch_index_from_wandb` we can either create our own index, or simply download the index stored as an artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "if not config.fetch_index_from_wandb:\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    wandb_callback.persist_index(index, index_name=\"camel-5b-hf-index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "if config.fetch_index_from_wandb:\n",
    "    storage_context = wandb_callback.load_storage_context(\n",
    "        artifact_url=\"sauravmaheshkar/llamaindex-local-models-index/camel-5b-hf-index:v0\"\n",
    "    )\n",
    "\n",
    "    # Load the index and initialize a query engine\n",
    "    index = load_index_from_storage(\n",
    "        storage_context,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Are Velociraptors pack hunters ?\")\n",
    "print(response, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_callback.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
