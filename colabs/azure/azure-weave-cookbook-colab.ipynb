{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/azure/azure-weave-cookbook-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{azure-weave-cookbook-colab} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install weave openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gpt-4-turbo\" # @param {type:\"string\"}\n",
    "model_id = \"mistral-7b-instruct-weave\"\n",
    "azure_model_option = \"openai\" # @param [\"openai\", \"ai_studio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_entity = \"a-sh0ts\" # @param {type:\"string\"}\n",
    "weave_project = \"azure-weave-cookbook\" # @param {type:\"string\"}\n",
    "eval_dataset_name = \"customer_service_inquiries\" # @param {type:\"string\"}\n",
    "publish_eval_data = True # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "from openai import AzureOpenAI, OpenAI\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_API_KEY')\n",
    "\n",
    "if azure_model_option == \"openai\":\n",
    "    os.environ[\"AZURE_OPENAI_ENDPOINT\"] = userdata.get('AZURE_OPENAI_ENDPOINT')\n",
    "    os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get('AZURE_OPENAI_API_KEY')\n",
    "    client = AzureOpenAI(\n",
    "        api_key=os.getenv(\"AZURE_API_KEY\"),\n",
    "        api_version=\"2024-02-01\",\n",
    "        azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    )\n",
    "elif azure_model_option == \"ai_studio\":\n",
    "    os.environ[\"AZURE_AI_STUDIO_API_ENDPOINT\"] = userdata.get('AZURE_AI_STUDIO_API_ENDPOINT')\n",
    "    os.environ[\"AZURE_AI_STUDIO_API_KEY\"] = userdata.get('AZURE_AI_STUDIO_API_KEY')\n",
    "\n",
    "    api_version = \"v1\"\n",
    "    client = OpenAI(\n",
    "        base_url=f\"{os.getenv('AZURE_AI_STUDIO_API_ENDPOINT')}/v1\",\n",
    "        api_key=os.getenv('AZURE_AI_STUDIO_API_KEY')\n",
    "    )\n",
    "else:\n",
    "    print(\"Please us one of the above options\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave\n",
    "weave.init(weave_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling Azure directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def call_azure_chat(model_id: str, messages: list, max_tokens: int = 1000, temperature: float = 0.5):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return {\"status\": \"success\", \"response\": response.choices[0].message.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def format_messages_for_mistral(messages: list):\n",
    "    system_message = messages[0][\"content\"]\n",
    "    formatted_messages = []\n",
    "\n",
    "    for message in messages[1:]:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            formatted_message = {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"[INST]\\n{system_message}\\n{message['content']}\\n[/INST]\"\n",
    "            }\n",
    "        else:\n",
    "            formatted_message = {\n",
    "                \"role\": message[\"role\"],\n",
    "                \"content\": message[\"content\"]\n",
    "            }\n",
    "        formatted_messages.append(formatted_message)\n",
    "\n",
    "    return formatted_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Create a snack recipe for a dish called the Azure Weav-e-ohs\"}\n",
    "]\n",
    "if \"mistral\" in model_id.lower():\n",
    "    messages = format_messages_for_mistral(messages)\n",
    "result = call_azure_chat(model_id, messages)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Functional LLM Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def format_prompt(prompt: str):\n",
    "    \"A formatting function for OpenAI models\"\n",
    "    system_prompt_formatted = \"You are a helpful assistant.\"\n",
    "\n",
    "    human_prompt = \"\"\"\n",
    "    {prompt}\n",
    "    \"\"\"\n",
    "\n",
    "    human_prompt_formatted = human_prompt.format(prompt=prompt)\n",
    "    messages = [{\"role\":\"system\", \"content\":system_prompt_formatted}, {\"role\":\"user\", \"content\":human_prompt_formatted}]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "def run_chat(model_id: str, prompt: str):\n",
    "    formatted_messages = format_prompt(prompt=prompt)\n",
    "    if \"mistral\" in model.lower():\n",
    "        formatted_messages = format_messages_for_mistral(formatted_messages)\n",
    "    result = call_azure_chat(model_id, formatted_messages, max_tokens=1000)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give a full recipe for a Weights & Biases inspired cocktail. Ensure you provide a list of ingredients, tools, and step by step instructions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_chat(model_id, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LLM Model Classes to iterate over hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class PromptTemplate:\n",
    "    system_prompt: str\n",
    "    human_prompt: str\n",
    "\n",
    "    @weave.op()\n",
    "    def format_prompt(self, email_content: str):\n",
    "        \"A formatting function for OpenAI models\"\n",
    "        system_prompt_formatted = self.system_prompt.format()\n",
    "        human_prompt_formatted = self.human_prompt.format(email_content=email_content)\n",
    "        messages = [{\"role\":\"system\", \"content\":system_prompt_formatted}, {\"role\":\"user\", \"content\":human_prompt_formatted}]\n",
    "        return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weave import Model\n",
    "from typing import Tuple\n",
    "\n",
    "class AzureEmailAssistant(Model):\n",
    "    model_id: str = model_id\n",
    "    prompt_template: PromptTemplate\n",
    "    max_tokens: int = 2048\n",
    "    temperature: float = 0.0\n",
    "\n",
    "    @weave.op()\n",
    "    def format_doc(self, doc: str) -> list:\n",
    "        \"Read and format the document\"\n",
    "        messages = self.prompt_template.format_prompt(doc)\n",
    "        return messages\n",
    "\n",
    "    @weave.op()\n",
    "    def respond(self, doc: str) -> dict:\n",
    "        \"Generate a response to the email inquiry\"\n",
    "        messages = self.format_doc(doc)\n",
    "        if \"mistral\" in self.model_id.lower():\n",
    "            messages = format_messages_for_mistral(messages)\n",
    "        output = call_azure_chat(\n",
    "            self.model_id,\n",
    "            messages=messages,\n",
    "            max_tokens=self.max_tokens,\n",
    "            temperature=self.temperature)\n",
    "        return output\n",
    "\n",
    "    @weave.op()\n",
    "    async def predict(self, email_content: str) -> str:\n",
    "        return self.respond(email_content)[\"response\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile customer_inquiry.txt\n",
    "Subject: Inquiry about Order Delay\n",
    "\n",
    "Hello,\n",
    "\n",
    "I placed an order last week for the new UltraGlow Skin Serum, but I have not received a shipping update yet. My order number is 12345. Could you please update me on the status of my shipment?\n",
    "\n",
    "Thank you,\n",
    "Jane Doe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "# Instructions\n",
    "You are a customer service response assistant. Our goal is to provide clear, concise, and polite responses to customer inquiries about products, shipping, and any issues they may have encountered. Some rules to remember:\n",
    "- Always be courteous and respectful.\n",
    "- Provide accurate and helpful information.\n",
    "- Responses should be concise and to the point.\n",
    "- Use formal language suitable for professional communication.\n",
    "## Formatting Rules\n",
    "Maintain a formal greeting and closing in each response. Do not use slang or overly casual language. Ensure all provided information is correct and double-check for typographical errors.\n",
    "\"\"\"\n",
    "\n",
    "human_prompt = \"\"\"\n",
    "Here is a customer inquiry received via email. Craft a suitable response based on the guidelines provided:\n",
    "<Customer Inquiry>\n",
    "{email_content}\n",
    "<End of Inquiry>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    system_prompt=system_prompt,\n",
    "    human_prompt=human_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave.init(weave_project) # Colab specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Path('customer_inquiry.txt').read_text()\n",
    "model = AzureEmailAssistant(model_id=model_id, prompt_template=prompt_template)\n",
    "response = model.respond(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Publish synthetically generated Evaluation data to Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if publish_eval_data:\n",
    "    from weave import Dataset\n",
    "    dataset = Dataset(name=eval_dataset_name, rows=[\n",
    "        {'id': '1', 'email_content': 'Subject: Inquiry about Order Delay\\n\\nHello,\\n\\nI placed an order last week for the new UltraGlow Skin Serum, but I have not received a shipping update yet. My order number is 12345. Could you please update me on the status of my shipment?\\n\\nThank you,\\nJane Doe'},\n",
    "        {'id': '2', 'email_content': 'Subject: Damaged Item Received\\n\\nHello,\\n\\nI received my order yesterday, but one of the items, a glass vase, was broken. My order number is 67890. How can I get a replacement or a refund?\\n\\nBest regards,\\nJohn Smith'},\n",
    "        {'id': '3', 'email_content': 'Subject: Wrong Item Delivered\\n\\nHi,\\n\\nI ordered a pair of blue sneakers, but I received a black pair instead. My order number is 54321. Could you please assist me with this issue?\\n\\nThank you,\\nEmily Johnson'},\n",
    "        {'id': '4', 'email_content': 'Subject: Request for Return Instructions\\n\\nDear Customer Service,\\n\\nI would like to return a dress I purchased last week as it does not fit well. My order number is 11223. Could you please provide the return instructions?\\n\\nSincerely,\\nLaura Davis'},\n",
    "        {'id': '5', 'email_content': 'Subject: Missing Items in Order\\n\\nHello,\\n\\nI just received my order, but two items are missing. My order number is 33445. Could you please help me resolve this?\\n\\nKind regards,\\nMichael Brown'},\n",
    "        {'id': '6', 'email_content': 'Subject: Delay in Order Confirmation\\n\\nDear Support Team,\\n\\nI placed an order two days ago but have not received a confirmation email yet. My order number is 99887. Can you confirm if my order was processed?\\n\\nThank you,\\nSarah Wilson'},\n",
    "        {'id': '7', 'email_content': 'Subject: Inquiry About Product Availability\\n\\nHi,\\n\\nI\\'m interested in purchasing the Professional Chef Knife Set, but it appears to be out of stock. Can you let me know when it will be available again?\\n\\nBest regards,\\nDavid Martinez'},\n",
    "        {'id': '8', 'email_content': 'Subject: Request for Invoice\\n\\nDear Customer Service,\\n\\nCould you please send me an invoice for my recent purchase? My order number is 55667. I need it for my records.\\n\\nThank you,\\nJessica Taylor'},\n",
    "        {'id': '9', 'email_content': 'Subject: Issue with Discount Code\\n\\nHello,\\n\\nI tried using the discount code SAVE20 during checkout, but it did not apply. My order number is 77654. Could you please assist me?\\n\\nSincerely,\\nRobert Anderson'},\n",
    "        {'id': '10', 'email_content': 'Subject: Request for Expedited Shipping\\n\\nHi,\\n\\nI need my order delivered urgently. Is it possible to upgrade to expedited shipping? My order number is 44556.\\n\\nThank you,\\nLinda Thompson'},\n",
    "        {'id': '11', 'email_content': 'Subject: Order Cancellation Request\\n\\nDear Support Team,\\n\\nI would like to cancel my recent order as I made a mistake while ordering. My order number is 33221. Can you please process the cancellation?\\n\\nBest regards,\\nWilliam Clark'}\n",
    "    ])\n",
    "    # Publish the dataset\n",
    "    weave.publish(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation whilst logging results to Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_uri = f\"weave:///{wandb_entity}/{weave_project}/object/{eval_dataset_name}:latest\"\n",
    "dataset = weave.ref(dataset_uri).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring function checking length of summary\n",
    "@weave.op()\n",
    "def check_conciseness(model_output: str) -> dict:\n",
    "    result = len(model_output.split()) < 300\n",
    "    return {'conciseness': result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(\n",
    "    dataset=dataset, scorers=[check_conciseness],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluation.evaluate(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
